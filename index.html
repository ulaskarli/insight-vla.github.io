<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>INSIGHT — Inference-time Sequence Introspection for Help-Triggering in VLAs</title>
  <meta name="description" content="INSIGHT: token-level uncertainty for timely human help in Vision-Language-Action rollouts."/>
  <meta property="og:title" content="INSIGHT — Help-Triggering in VLAs"/>
  <meta property="og:description" content="Token-level uncertainty → timely human help for safer, more reliable VLA rollouts."/>
  <meta property="og:type" content="website"/>
  <meta property="og:image" content="static/images/teaser.jpg"/>
  <link rel="icon" href="static/images/favicon.ico"/>

  <style>
    /* ---- light, flowing, OpenVLA-like ---- */
    :root{
      --bg:#fff; --ink:#121212; --muted:#5b5b5b; --accent:#0b74d1;
      --maxw: 1060px; --gap: 28px;
    }
    html,body{background:var(--bg);color:var(--ink);font-family: ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,"Helvetica Neue",Arial}
    a{color:var(--accent);text-decoration:none} a:hover{text-decoration:underline}
    .wrap{max-width:var(--maxw);margin:0 auto;padding:28px 20px 80px}

    /* nav */
    nav{position:sticky;top:0;background:#fff;z-index:10;border-bottom:1px solid #eee}
    nav .navwrap{max-width:var(--maxw);margin:0 auto;display:flex;gap:18px;align-items:center;padding:10px 20px}
    nav a{color:var(--ink);padding:6px 8px;border-radius:6px}
    nav a:hover{background:#f6f8fb}

    /* type + spacing */
    h1{font-size:clamp(32px,5vw,52px);line-height:1.15;margin:.2em 0 .25em}
    h2{font-size:clamp(24px,3.2vw,36px);line-height:1.2;margin:1.6em 0 .6em}
    h3{font-size:1.2rem;margin:1.1em 0 .4em}
    p{line-height:1.7}
    .subtitle{color:var(--muted);font-size:1.05rem}

    /* hero buttons */
    .badges{display:flex;flex-wrap:wrap;gap:10px;margin-top:14px}
    .btn{display:inline-block;padding:10px 14px;border-radius:8px;background:#eef3f8;border:1px solid #d9e2ec;color:var(--ink)}

    /* sections */
    section{margin-top:48px}
    figure{margin:0}
    figcaption{color:var(--muted);font-size:.95rem;margin-top:8px}

    /* media */
    .fullvideo video{width:100%;height:auto;border-radius:12px}
    .method-grid{display:grid;grid-template-columns:1fr;gap:var(--gap)}
    @media (min-width: 900px){ .method-grid{grid-template-columns:1fr 1fr} }

    /* results sub-nav */
    .results-nav{display:flex;flex-wrap:wrap;gap:8px;margin:-6px 0 14px 0}
    .results-nav .btn{background:#f6f8fb;border-color:#e6edf3}

    /* table */
    .table-card{background:#fff;border:1px solid #ddd;border-radius:10px;overflow:auto}
    .table-card table{width:100%;border-collapse:collapse;min-width:720px}
    .table-card th,.table-card td{padding:10px 12px;text-align:center;border-bottom:1px solid #eee;white-space:nowrap}
    .table-card th:first-child,.table-card td:first-child{text-align:left}
    .table-card thead th{font-weight:700;background:#f3f5f7;color:#333}
    .kpi-up{color:#156f00;font-weight:700}
    .kpi-down{color:#0054a6;font-weight:700}

    /* rollouts (simple flow) */
    #filters{display:flex;flex-wrap:wrap;gap:10px;margin:6px 0 16px}
    #filters select{background:#fff;color:var(--ink);border:1px solid #ccc;border-radius:6px;padding:8px}
    #rollout-list{display:grid;grid-template-columns:1fr;gap:28px}
    .rollout video{width:100%;height:auto;border-radius:10px}
    .meta{color:#666;margin-top:6px}
  </style>
</head>
<body>
  <nav>
    <div class="navwrap">
      <a href="#top"><strong>INSIGHT</strong></a>
      <a href="#demo">Video</a>
      <a href="#abstract">Abstract</a>
      <a href="#method">Method</a>
      <a href="#results">Results</a>
      <a href="#rollouts">Rollouts</a>
      <a href="#bibtex">BibTeX</a>
      <a href="#team">Team</a>
    </div>
  </nav>

  <main class="wrap">
    <!-- HEADLINE -->
    <section id="top">
      <h1>INSIGHT: Inference-time Sequence Introspection for Help-Triggering in VLAs</h1>
      <p class="subtitle">Token-level uncertainty → timely human help for safer, more reliable Vision–Language–Action rollouts.</p>
      <div class="badges">
        <a class="btn" href="https://github.com/ulaskarli/insight-vla-help-triggers"><strong>Code</strong></a>
        <a class="btn" href="https://arxiv.org/abs/TO-BE-ADDED">arXiv</a>
        <a class="btn" href="#bibtex">BibTeX</a>
        <a class="btn" href="mailto:ulasberk.karli@yale.edu">Contact</a>
      </div>
      <p class="subtitle" style="margin-top:8px">Ulas Berk Karli · Ziyao Shangguan · Tesca Fitzgerald</p>
    </section>

    <!-- DEMO VIDEO (plays with sound on click) -->
    <section id="demo" class="fullvideo">
      <video src="static/videos/icra26_video.mp4" poster="static/images/icra_teaser.svg" controls playsinline preload="metadata"></video>
    </section>

    <!-- ABSTRACT -->
    <section id="abstract">
      <h2>Abstract</h2>
      <p>Recent Vision–Language–Action (VLA) models exhibit strong generalization but lack mechanisms to anticipate failures and request timely human help. <strong>INSIGHT</strong> introduces a learning framework that leverages token-level uncertainty signals to predict when intervention is needed. Built on top of π₀-FAST, our approach extracts entropy, log-probability, and Dirichlet-based estimates of aleatoric and epistemic uncertainty, then trains compact transformer classifiers to map these sequences to help triggers. We study both strong and weak supervision regimes, comparing their effectiveness across in-distribution and out-of-distribution tasks. Strong labels capture fine-grained uncertainty dynamics for reliable detection, while weak labels, though noisier, remain competitive when training and evaluation align—offering scalability where dense annotation is impractical. Crucially, modeling temporal evolution of token-level uncertainty yields far greater predictive power than static sequence-level scores. This work provides the first systematic evaluation of uncertainty-based introspection in VLAs, paving the way for active learning and real-time error mitigation through selective human intervention.</p>
    </section>

    <!-- METHOD -->
    <section id="method">
      <h2>Method</h2>
      <div class="method-grid">
        <figure>
          <img src="static/images/icra_teaser.svg" alt="Method: token distributions → uncertainty features → transformer → help/no-help">
          <figcaption>INSIGHT consumes token distributions from a VLA (e.g., π₀-FAST), computes uncertainty features, and predicts help triggers online.</figcaption>
        </figure>
        <div>
          <ul>
            <li><strong>Inputs:</strong> per-token distributions → entropy, −log p, aleatoric (AU), epistemic (EU).</li>
            <li><strong>Decision:</strong> compact transformer over short windows → help/no-help at step t.</li>
            <li><strong>Training:</strong> <em>Strong</em> (step labels) and <em>Weak</em> (episode labels, MIL) with CP baselines.</li>
            <li><strong>Deployment:</strong> triggers enable teleop or preference queries; fewer interrupts on successes, earlier warnings on fails.</li>
          </ul>
        </div>
      </div>
    </section>

    <!-- RESULTS -->
    <section id="results">
      <h2>Results</h2>

      <div class="results-nav">
        <a class="btn" href="#res-id">In-Distribution</a>
        <a class="btn" href="#res-ds">Distribution-Shift</a>
        <a class="btn" href="#res-lid">Large In-Distribution</a>
        <a class="btn" href="#res-ood">Sim-OOD</a>
        <a class="btn" href="#res-timing">Help Timing</a>
      </div>

      <section id="res-id">
        <h3>In-Distribution Performance</h3>
        <figure>
          <img src="static/images/ID_sign.svg" alt="In-distribution boxplots comparing INSIGHT vs CP baselines across folds.">
          <figcaption>Results for the transformer (INSIGHT) and Conformal Prediction based on entropy (CP-E) and perplexity (CP-P). Each box plot indicates mean (dashed) and median (solid) across folds; error bars are ±1 sd. Significance by paired Wilcoxon (two-sided): * p&lt;0.05, ** p&lt;0.01.</figcaption>
        </figure>
      </section>

      <section id="res-ds">
        <h3>Distribution-Shift Performance</h3>
        <figure>
          <img src="static/images/OOD.svg" alt="Distribution-shift boxplots across folds.">
          <figcaption>Same caption as above for boxplots (INSIGHT vs CP-E/CP-P, mean/median, ±1 sd, Wilcoxon significance).</figcaption>
        </figure>
      </section>

      <section id="res-lid">
        <h3>Large In-Distribution Performance</h3>
        <figure>
          <img src="static/images/LID.svg" alt="Large in-distribution boxplots across folds.">
          <figcaption>Same caption as above for boxplots.</figcaption>
        </figure>
      </section>

      <section id="res-ood">
        <h3>Simulation-OOD</h3>
        <figure>
          <img src="static/images/Sim_OOD.svg" alt="Simulation OOD performance summary.">
          <figcaption>Performance under simulated OOD setup.</figcaption>
        </figure>
      </section>

      <section id="res-timing">
        <h3>Help Timing & Frequency</h3>
        <div class="table-card">
          <table>
            <thead>
              <tr>
                <th>Method</th>
                <th>TTFH (fail) <span class="kpi-down">↓</span></th>
                <th>Triggers<sub>succ</sub> <span class="kpi-down">↓</span></th>
                <th>Triggers<sub>fail</sub> (≥ 1 ok)</th>
                <th>Trigger Rate (success) <span class="kpi-down">↓</span></th>
                <th>Trigger Rate (fail) <span class="kpi-up">↑</span></th>
              </tr>
            </thead>
            <tbody>
              <tr><td>CP-W (Entropy)</td><td>6.891 ± 2.257</td><td>0.457 ± 0.302</td><td>1.721 ± 0.739</td><td>0.031 ± 0.020</td><td>0.118 ± 0.050</td></tr>
              <tr><td><strong>Strong Superv.</strong></td><td><strong>5.597 ± 0.809</strong></td><td>0.710 ± 0.440</td><td><strong>7.062 ± 1.225</strong></td><td>0.047 ± 0.029</td><td><strong>0.472 ± 0.081</strong></td></tr>
              <tr><td>Weak Superv.</td><td>7.929 ± 1.867</td><td><strong>0.122 ± 0.172</strong></td><td>1.566 ± 1.025</td><td><strong>0.008 ± 0.011</strong></td><td>0.105 ± 0.069</td></tr>
            </tbody>
          </table>
        </div>
      </section>
    </section>

    <!-- ROLLOUTS -->
    <section id="rollouts">
      <h2>Rollouts</h2>
      <p class="subtitle" style="margin-top:-6px">Filter by task, dataset, and outcome; click to play with audio.</p>

      <div id="filters">
        <select data-filter="task">
          <option value="">All Tasks</option><option>Lift</option><option>Place</option><option>Push</option>
        </select>
        <select data-filter="setting">
          <option value="">All Datasets</option><option>ID</option><option>Shift</option><option>Sim-OOD</option>
        </select>
        <select data-filter="outcome">
          <option value="">All Outcomes</option><option>Success</option><option>Fail</option>
        </select>
        <select data-filter="model">
          <option value="">All Models</option><option>Strong</option><option>Weak</option><option>CP-E</option><option>CP-P</option>
        </select>
      </div>

      <div id="rollout-list"></div>
    </section>

    <!-- BibTeX (keep simple for now) -->
    <section id="bibtex">
      <h2>BibTeX</h2>
      <pre style="white-space:pre-wrap;background:#f6f8fb;border:1px solid #e6edf3;border-radius:12px;padding:16px;overflow:auto">
@inproceedings{TODO2026insight,
  title={INSIGHT: Inference-time Sequence Introspection for Help-Triggering in Vision-Language-Action Models},
  author={Karli, Ulas Berk and Shangguan, Ziyao and Fitzgerald, Tesca},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  year={2026}
}
      </pre>
    </section>

    <section id="team">
      <h2>Team</h2>
      <p>Ulas Berk Karli (Yale), Ziyao Shangguan (Yale), Tesca Fitzgerald (Yale).</p>
    </section>
  </main>

  <script>
    // Simple rollout renderer (flowing list)
    const data = [
      {src:"static/videos/lift_ID_success_STRONG_ep42_ttfh3.mp4", thumb:"static/images/lift_ID_ep42.jpg",
       task:"Lift", setting:"ID", outcome:"Success", model:"Strong", ttFH:3,
       notes:"Early help stabilized grasp."},
      {src:"static/videos/place_Shift_fail_WEAK_ep07_ttfh2.mp4", thumb:"static/images/place_Shift_ep07.jpg",
       task:"Place", setting:"Shift", outcome:"Fail", model:"Weak", ttFH:2,
       notes:"Late trigger; contact slip."}
    ];
    const list = document.getElementById('rollout-list');
    function renderRollouts(){
      const sels=[...document.querySelectorAll('#filters [data-filter]')];
      const filtered = data.filter(d=>sels.every(sel=>{const k=sel.dataset.filter; return !sel.value || d[k]===sel.value;}));
      list.innerHTML = filtered.map(d=>`
        <div class="rollout">
          <figure>
            <video src="${d.src}" poster="${d.thumb}" controls playsinline preload="metadata"></video>
            <figcaption>${d.task} • ${d.setting} • ${d.model} • ${d.outcome}
              <div class="meta">TTFH=${d.ttFH} — ${d.notes||""}</div>
            </figcaption>
          </figure>
        </div>`).join('');
    }
    renderRollouts();
    document.getElementById('filters').addEventListener('change', renderRollouts);
  </script>
</body>
</html>