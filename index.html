<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>INSIGHT: Inference-time Sequence Introspection for VLAs</title>
    <meta name="description"
        content="INSIGHT: token-level uncertainty for timely human help in Vision-Language-Action rollouts.">
    <meta property="og:title" content="INSIGHT — Help-Triggering in VLAs">
    <meta property="og:description"
        content="Token-level uncertainty → timely human help for safer, more reliable VLA rollouts.">
    <meta property="og:type" content="website">
    <meta property="og:image" content="static/images/teaser.jpg">
    <link rel="icon" href="static/images/favicon.ico">

    <!-- Fonts and CSS -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <!-- Scripts -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>


    <style>
        .eyebrow {
            text-transform: uppercase;
            letter-spacing: .06em;
            font-size: .8rem;
            color: #6b7280;
        }

        .pill {
            display: inline-block;
            padding: 2px 8px;
            border: 1px solid #d9e2ec;
            border-radius: 999px;
            font-size: .8rem;
            color: #374151;
            background: #f6f8fb;
            margin-right: 6px
        }

        .soft {
            background: #f9fafb;
            border: 1px solid #edf2f7;
            border-radius: 12px;
            padding: 16px
        }

        .twocol {
            display: grid;
            grid-template-columns: 1fr;
            gap: 18px
        }

        @media(min-width:980px) {
            .twocol {
                grid-template-columns: 1.15fr .85fr
            }
        }

        .cap {
            color: #6b7280;
            font-size: .95rem;
            margin-top: 6px
        }

        .list-tight li {
            margin: 6px 0
        }

        /* Results section spacing tweaks */
        section .title.is-4 {
        margin-top: 1.2em;   /* space above sub-section titles */
        margin-bottom: 0.6em; /* space below before content */
        }

        section .title.is-5 {
        margin-top: 1em;
        margin-bottom: 0.5em;
        }

        figure {
        margin-bottom: 1.4em; /* more breathing room under each figure */
        }

        .content p {
        margin-bottom: 1em; /* paragraphs inside results/discussion don’t collide */
        }
    </style>

</head>

<body>
    <!-- Hero Section -->
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">INSIGHT:<br><span
                                style="font-size:2.4rem;">Inference-time Sequence Introspection for Help-Triggering in
                                VLAs</span></h1>

                        <!-- Authors -->
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://ulaskarli.github.io">Ulas Berk Karli</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://ziyaosg.github.io">Ziyao Shangguan</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.tescafitzgerald.com">Tesca Fitzgerald</a><sup>1</sup>
                            </span>
                            <br>
                            <span class="author-block">
                                <sup>1</sup><a href="https://iqr.cs.yale.edu">Inquisitive Robotics Lab</a>,
                            </span>
                            <span class="author-block">Yale University</span>
                        </div>

                        <!-- Links -->
                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2510.01389"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon"><i class="fas fa-file-pdf"></i></span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://github.com/ulaskarli/insight-vla-help-triggers"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon"><i class="fab fa-github"></i></span>
                                        <span>Code</span>
                                    </a>
                                </span>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Teaser Video -->
    <section class="hero teaser teaser-video">
    <div class="container is-max-desktop has-text-centered">
        <div class="hero-body">
        <div style="position:relative;display:inline-block;width:80%">
            <video id="teaser" autoplay loop playsinline controls style="width:100%;border-radius:12px">
            <source src="static/videos/icra26_video.mp4" type="video/mp4">
            </video>
            <div id="unmute-tip" 
                style="position:absolute;bottom:10px;right:10px;
                        background:rgba(0,0,0,0.6);color:#fff;
                        padding:6px 10px;border-radius:6px;
                        font-size:0.9rem;">
            ▶ Click speaker icon to unmute
            </div>
        </div>
        </div>
    </div>
    </section>

    <script>
    // Hide the tip after a few seconds or when user interacts
    const tip = document.getElementById('unmute-tip');
    const video = document.getElementById('teaser');
    if (video && tip){
        setTimeout(()=> tip.style.display='none', 4000);
        video.addEventListener('volumechange', ()=> tip.style.display='none');
        video.addEventListener('play', ()=> tip.style.display='none');
    }
    </script>

    <!-- Abstract Section -->
    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3">Abstract</h2>
            <p class="content has-text-justified">
                Recent Vision-Language-Action (VLA) models show strong generalization capabilities, yet they lack
                introspective mechanisms for anticipating failures and requesting help from a human supervisor. We
                present <b>INSIGHT</b>, a learning framework for leveraging token-level uncertainty signals to predict
                when a VLA should request help. Using &pi;<sub>0</sub>-FAST as the underlying model, we extract per-token
                <i>entropy</i>, <i>log-probability</i>, and Dirichlet-based estimates of <i>aleatoric and epistemic
                    uncertainty</i>, and train compact transformer classifiers to map these sequences to help triggers.
                We explore supervision regimes for strong or weak supervision, and extensively compare them across
                in-distribution and out-of-distribution tasks. Our results show a trade-off: strong labels enable models
                to capture fine-grained uncertainty dynamics for reliable help detection, while weak labels, though
                noisier, still support competitive introspection when training and evaluation are aligned, offering a
                scalable path when dense annotation is impractical. Crucially, we find that modeling the temporal
                evolution of token-level uncertainty signals with transformers provides far greater predictive power
                than static sequence-level scores. This study provides the first systematic evaluation of
                uncertainty-based introspection in VLAs, opening future avenues for active learning and for real-time
                error mitigation through selective human intervention.
            </p>
        </div>
    </section>
    <!-- Problem -->
    <section id="Problem" class="section">
    <div class="container is-max-desktop">
        <div class="eyebrow">Problem</div>
        <h2 class="title is-3">Inference in VLAs</h2>

        <p class="content has-text-justified">
        Our work builds upon <b>&pi;<sub>0</sub>-FAST</b>, a VLA model that takes as input a natural language instruction, 
        RGB images of the environment, and the robot’s state. The model processes these inputs in a forward pass and auto-regressively generates 
        a sequence of tokens. These tokens are then decoded into continuous joint actions, which the robot executes in the environment.
        </p>

        <figure>
        <img src="static/images/pi0fast_inference.gif" alt="Inference process of PI0-FAST">
        </figure>

        <p class="content has-text-justified">
        Importantly, each token is drawn from its own probability distribution over the action vocabulary. This exposes <i>token-level uncertainty signals</i>, 
        such as entropy, negative log-likelihood, or Dirichlet-based measures. Our central research question is therefore: 
        <b>Can token-level uncertainty reliably indicate when the robot should request human help?</b>
        </p>

        <figure>
        <img src="static/images/rq.gif" alt="Inference process GIF showing token-level probability distributions">
        </figure>

    </div>
    </section>

    <!-- episode vs step vs token -->
    <section id="granularity" class="section">
        <div class="container is-max-desktop">
            <div class="eyebrow">Granularity of signals</div>
            <h2 class="title is-3">Episode vs Step vs Token</h2>
                    <p class="cap">
        With the inference loop in mind, we next clarify the three levels of granularity—episode, step, and token—used throughout our analysis.
        </p>
            <div class="twocol">
                <figure>
                    <!-- Use your figure from the paper -->
                    <img src="static/images/level_decomposition.svg"
                        alt="Episode, Step, Token levels and where uncertainty lives.">
                    <figcaption class="cap">INSIGHT taps uncertainty <em>at the token level</em> from the VLA,
                        then reasons over temporal windows to trigger help.</figcaption>
                </figure>

                <div class="content">
                    <p>
                    <span class="pill">Token</span> is the infinitesimal unit produced during inference in VLAs. 
                    A VLA generates <i>n</i> tokens, each drawn from its own probability distribution over the vocabulary. 
                    </p>

                    <p>
                    <span class="pill">Action</span> involves the <i>n</i> tokens, which are not fixed in number per
                     for &pi;<sub>0</sub>-FAST, decoded in to a chunk of robot actions.
                    </p>

                    <p>
                    <span class="pill">Step</span> consists of one cycle of collecting an observation, performing inference, 
                    decoding the resulting token sequence into an action chunk, and executing it. 
                    </p>

                    <p>
                    <span class="pill">Episode</span> refers to one complete rollout (success or failure), which consists of multiple inferences by the VLA. 
                    Formally, an episode includes all <i>K</i> steps:
                    </p>

                    <p style="text-align:center; font-family:monospace;">
                    E = (a<sup>1:H</sup><sub>1</sub>, a<sup>1:H</sup><sub>2</sub>, …, a<sup>1:H</sup><sub>K</sub>)
                    </p>
                    <div class="soft">
                        <strong>Key idea:</strong> model the <em>temporal evolution</em> of token-level uncertainty with
                        a compact transformer → more reliable help triggers.
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Transformer -->
    <section id="models" class="section">
        <div class="container is-max-desktop">
            <div class="eyebrow">Modeling</div>
            <h2 class="title is-3">Training Compact Transformers for Introspection</h2>
<p class="cap">
        INSIGHT operates on token-level signals over time. We formalize this with a lightweight transformer that predicts when to request help.
        </p>
            <figure>
                <img src="static/images/icra_transformer_website.svg" alt="Compact transformer diagram">
                <figcaption class="cap">A lightweight transformer over uncertainty sequences.</figcaption>
            </figure>

            <div>
                <ul class="list-tight">
                    <li><strong>Inputs:</strong> token-level features per step: entropy, −log p, Dirichlet AU/EU.
                    </li>
                    <li><strong>Backbone:</strong> 1–2 layers, small hidden size; positional encodings for time.
                    </li>
                    <li><strong>Objective:</strong> predict <em>help/no-help</em> for the window’s end step.</li>
                </ul>
                <div class="soft">
                    <strong>Why a small transformer?</strong> Token features are informative—the introspector transformer can stay
                    <em>compact</em> for fast, online triggering.
                </div>
            </div>
            
        </div>
    </section>

    <!-- Supervison labels -->
    <section id="labels" class="section">
        <div class="container is-max-desktop">
            <div class="eyebrow">Supervision</div>
            <h2 class="title is-3">Strong vs Weak Labels</h2>
            <p class="cap">
        The effectiveness of introspection depends on supervision of the compact transformer. We compare dense step-level (strong) and scalable episode-level (weak) labels.
        </p>
            <div class="columns is-variable is-6">
                <div class="column">
                    <h3 class="title is-5">Strong (step labels)</h3>
                    <p>Dense, per-step “help/no-help” supervision. Captures the onset and decay of uncertainty around
                        challenging events.</p>
                    <figure>
                        <img src="static/images/strong_super.gif" alt="Strong label example GIF">
                        <figcaption class="cap">Each step in an episode is labeled as needing help or not</figcaption>
                    </figure>
                    <ul class="list-tight">
                        <li>Pros: precise timing, best early detection.</li>
                        <li>Cons: costly to annotate densely.</li>
                    </ul>
                </div>
                <div class="column">
                    <h3 class="title is-5">Weak (episode labels / MIL)</h3>
                    <p>Only episode-level success/fail. We use multiple-instance learning to supervise the transformer.</p>
                    <figure>
                        <img src="static/images/weak_superv.gif" alt="Weak label example GIF">
                        <figcaption class="cap">Each episode is labeled as success or failure</figcaption>
                    </figure>
                    <ul class="list-tight">
                        <li>Pros: scalable, easy to collect.</li>
                        <li>Cons: noisier; performance decays when train/eval granularity do not align.</li>
                    </ul>
                </div>
            </div>
        </div>
    </section>

    <!--Datasets-->
    <section id="datasets" class="section">
        <div class="container is-max-desktop">
            <div class="eyebrow">Datasets</div>
            <h2 class="title is-3">Evaluation Settings</h2>

            <div class="content">
                <p>We evaluate across three settings to probe generalization and robustness:</p>
            </div>

            <div class="soft">
                <div class="columns is-multiline is-variable is-4">
                <div class="column is-4">
                <h3 class="title is-6">In-Distribution</h3>
                <p class="is-size-6">
                    Lift and Pick-Place variants in the <b>Kitchen (real-world)</b> environment. 
                    Objects and backgrounds match those seen during training.
                </p>
                </div>

                <div class="column is-4">
                <h3 class="title is-6">Distribution Shift</h3>
                <p class="is-size-6">
                    Evaluation in the Kitchen with <i>distribution shifts</i>: new distractor objects, 
                    changes in shape, color, location, and orientation. Tests robustness to visual variation 
                    while staying in the same environment.
                </p>
                </div>

                <div class="column is-4">
                <h3 class="title is-6">Simulated OOD</h3>
                <p class="is-size-6">
                    Cross-domain evaluation between <b>real Kitchen</b> and <b>simulated LIBERO-10</b>. 
                    Objects and tasks differ completely, and the underlying &pi;<sub>0</sub>-FAST checkpoints differ as well 
                    (Kitchen-trained vs. LIBERO-trained). This setting represents true out-of-distribution generalization.
                </p>
                </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Rollouts Section with dropdown filters -->
    <section id="rollouts" class="section">
    <div class="container is-max-desktop">
        <h2 class="title is-3">Rollouts</h2>
        <p class="content has-text-justified" style="margin-bottom:18px">
        If you would like to explore examples from our datasets, this interactive section allows you to 
        browse rollout videos filtered by <b>dataset</b> (In-Distribution, Distribution Shift, or Simulation-OOD) 
        and by <b>outcome</b> (Success or Failure). These examples illustrate how fine-tuned &pi;<sub>0</sub>-FAST 
        behaves across different settings and why uncertainty-aware help detection is necessary.
        </p>
            <div class="field is-grouped is-grouped-multiline" style="margin-bottom:14px">
                <div class="control">
                    <div class="select is-link is-small">
                        <select id="datasetSel" aria-label="Dataset filter">
                            <option value="">All Datasets</option>
                            <option value="ID">ID</option>
                            <option value="Shift">Shift</option>
                            <option value="Sim-OOD">Sim-OOD</option>
                        </select>
                    </div>
                </div>
                <div class="control">
                    <div class="select is-link is-small">
                        <select id="outcomeSel" aria-label="Outcome filter">
                            <option value="">All Outcomes</option>
                            <option value="Success">Success</option>
                            <option value="Fail">Fail</option>
                        </select>
                    </div>
                </div>
            </div>

            <!-- Grid -->
            <div id="rolloutGrid" class="columns is-multiline is-variable is-4"></div>

            <p class="has-text-grey is-size-6" id="rolloutEmpty" style="display:none;margin-top:12px">
                No rollouts match the selected filters.
            </p>
        </div>
    </section>

    <script>
        // ===== Rollout data (replace with your real files) =====
        const ROLLOUTS = [
            // ID
            { src: "static/videos/l_c_20_S.mp4", thumb: "static/videos/l_c_20_S.mp4", dataset: "ID", outcome: "Success", caption: "Lift the Corn • ID • Success" },
            { src: "static/videos/l_c_19_F.mp4", thumb: "static/videos/l_c_19_F.mp4", dataset: "ID", outcome: "Fail", caption: "Lift the Corn • ID • Fail" },
            { src: "static/videos/p_p_s_11_S.mp4", thumb: "static/videos/p_p_s_11_S.mp4", dataset: "ID", outcome: "Success", caption: "Put the Pot in the Sink • ID • Success" },
            { src: "static/videos/p_p_s_16_F.mp4", thumb: "static/videos/p_p_s_16_F.mp4", dataset: "ID", outcome: "Fail", caption: "Put the Pot in the Sink • ID • Fail" },
            { src: "static/videos/p_c_p_9_S.mp4", thumb: "static/videos/p_c_p_9_S.mp4", dataset: "ID", outcome: "Success", caption: "Put the Corn in the Pot • ID • Success" },
            { src: "static/videos/p_c_p_13_F.mp4", thumb: "static/videos/p_c_p_13_F.mp4", dataset: "ID", outcome: "Fail", caption: "Put the Corn in the Pot • ID • Fail" },
            { src: "static/videos/l_e_9_S.mp4", thumb: "static/videos/l_e_9_S.mp4", dataset: "ID", outcome: "Success", caption: "Lift the Eggplant • ID • Success" },
            { src: "static/videos/l_e_15_F.mp4", thumb: "static/videos/l_e_15_F.mp4", dataset: "ID", outcome: "Fail", caption: "Lift the Eggplant • ID • Fail" },
            // Shift
            { src: "static/videos/l_c_135_S.mp4", thumb: "static/videos/l_c_135_S.mp4", dataset: "Shift", outcome: "Success", caption: "Lift the Corn • Distribution Shift • Success" },
            { src: "static/videos/l_c_151_F.mp4", thumb: "static/videos/l_c_151_F.mp4", dataset: "Shift", outcome: "Fail", caption: "Lift the Corn • Distribution Shift • Fail" },
            { src: "static/videos/p_p_s_82_S.mp4", thumb: "static/videos/p_p_s_82_S.mp4", dataset: "Shift", outcome: "Success", caption: "Put the Pot in the Sink • Distribution Shift • Success" },
            { src: "static/videos/p_p_s_90_F.mp4", thumb: "static/videos/p_p_s_90_F.mp4", dataset: "Shift", outcome: "Fail", caption: "Put the Pot in the Sink • Distribution Shift • Fail" },
            { src: "static/videos/p_c_p_52_S.mp4", thumb: "static/videos/p_c_p_52_S.mp4", dataset: "Shift", outcome: "Success", caption: "Put the Corn in the Pot • Distribution Shift • Success" },
            { src: "static/videos/p_c_p_62_F.mp4", thumb: "static/videos/p_c_p_62_F.mp4", dataset: "Shift", outcome: "Fail", caption: "Put the Corn in the Pot • Distribution Shift • Fail" },
            { src: "static/videos/l_e_84_S.mp4", thumb: "static/videos/l_e_84_S.mp4", dataset: "Shift", outcome: "Success", caption: "Lift the Eggplant • Distribution Shift • Success" },
            { src: "static/videos/l_e_75_F.mp4", thumb: "static/videos/l_e_75_F.mp4", dataset: "Shift", outcome: "Fail", caption: "Lift the Eggplant • Distribution Shift • Fail" },
            // Sim-OOD
            { src: "static/videos/10_4_30_S.mp4", thumb: "static/videos/10_4_30_S.mp4", dataset: "Sim-OOD", outcome: "Success", caption: "Libero 10 Task 4 • Sim-OOD • Success" },
            { src: "static/videos/10_4_38_F.mp4", thumb: "static/videos/10_4_38_F.mp4", dataset: "Sim-OOD", outcome: "Fail", caption: "Libero 10 Task 4 • Sim-OOD • Fail" },
            { src: "static/videos/10_5_16_S.mp4", thumb: "static/videos/10_5_16_S.mp4", dataset: "Sim-OOD", outcome: "Success", caption: "Libero 10 Task 5 • Sim-OOD • Success" },
            { src: "static/videos/10_5_11_F.mp4", thumb: "static/videos/10_5_11_F.mp4", dataset: "Sim-OOD", outcome: "Fail", caption: "Libero 10 Task 5 • Sim-OOD • Fail" },
            { src: "static/videos/10_7_18_S.mp4", thumb: "static/videos/10_7_18_S.mp4", dataset: "Sim-OOD", outcome: "Success", caption: "Libero 10 Task 7 • Sim-OOD • Success" },
            { src: "static/videos/10_7_5_F.mp4", thumb: "static/videos/10_7_5_F.mp4", dataset: "Sim-OOD", outcome: "Fail", caption: "Libero 10 Task 7 • Sim-OOD • Fail" },
            { src: "static/videos/10_9_17_S.mp4", thumb: "static/videos/10_9_17_S.mp4", dataset: "Sim-OOD", outcome: "Success", caption: "Libero 10 Task 9 • Sim-OOD • Success" },
            { src: "static/videos/10_9_8_F.mp4", thumb: "static/videos/10_9_8_F.mp4", dataset: "Sim-OOD", outcome: "Fail", caption: "Libero 10 Task 9 • Sim-OOD • Fail" },
        ];

        // ===== Render & Filter =====
        const grid = document.getElementById('rolloutGrid');
        const emptyMsg = document.getElementById('rolloutEmpty');
        const datasetSel = document.getElementById('datasetSel');
        const outcomeSel = document.getElementById('outcomeSel');

        function renderRollouts() {
            const ds = datasetSel.value;
            const oc = outcomeSel.value;
            const items = ROLLOUTS.filter(r => (!ds || r.dataset === ds) && (!oc || r.outcome === oc));

            grid.innerHTML = items.map(r => `
        <div class="column is-6">
          <figure class="image">
            <video src="${r.src}" poster="${r.thumb || ''}" controls playsinline preload="metadata" style="border-radius:12px;width:100%"></video>
            <figcaption class="has-text-grey" style="margin-top:6px">${r.caption || `${r.dataset} • ${r.outcome}`}</figcaption>
          </figure>
        </div>
      `).join('');

            emptyMsg.style.display = items.length ? 'none' : 'block';
        }

        if (datasetSel && outcomeSel && grid) {
            // Default filter selection
            datasetSel.value = "ID";   // or "Shift" / "Sim-OOD"
            outcomeSel.value = "Success"; // optional
            datasetSel.addEventListener('change', renderRollouts);
            outcomeSel.addEventListener('change', renderRollouts);
            renderRollouts();
        }
    </script>

    <!-- Results Section (placeholder for boxplots and tables) -->
    <section class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3">Results</h2>
            <p class="cap">
            Results for the transformer (INSIGHT) and Conformal Prediction based on entropy (CP-E) and
            perplexity (CP-P). Boxplots show mean (dashed) and median (solid) across folds; error bars denote ±1 SD.
            Paired Wilcoxon significance: * p&lt;0.05, ** p&lt;0.01. Higher is better unless noted.
            </p>

            <div class="content">
            <h4 class="title is-5">Do uncertainty metrics provide predictive power for requesting help?</h4>
            <p>
            <p>
            Yes. Across all experiments, token-level uncertainty signals (entropy, log-probability, aleatoric and epistemic uncertainty) 
            offered predictive power beyond random guessing (accuracy/F1 > 0.5).
            </p>

            <h3 class="title is-4">In-Distribution Performance</h3>
            <figure>
                <img src="static/images/ID_sign.svg" alt="Boxplots of accuracy, F1 across 10 folds.">
                <div class="content" style="margin-top:10px">
                <ul class="is-size-6">
                    <li><b>Setting.</b> Calibration, and testing all come from the same distribution, 
                        so Conformal Prediction's coverage guarantees formally hold.</li>
                    <li><b>Signal exists.</b> Token-level uncertainty (entropy, log-p, AU, EU) carries predictive 
                        power beyond random (accuracy/F1 &gt; 0.5).</li>
                    <li><b>Baselines vs. INSIGHT.</b> CP—though sequence-level—sometimes reaches ~0.7 under 
                        <i>weak-label</i> evaluation when calibration and evaluation align, but temporal transformers 
                        remain stronger overall.</li>
                </ul>
                </div>
            </figure>

            <h4 class="title is-5">How transferable are models under distribution shift?</h4>
            <p>
                All models degraded under distribution shift. Strongly supervised models degraded least.
            </p>

            <h3 class="title is-4">Distribution Shift Performance</h3>
            <figure>
                <img src="static/images/OOD.svg" alt="Boxplots under distribution shift.">
                <div class="content" style="margin-top:10px">
                <ul class="is-size-6">
                    <li><b>Setting.</b> Object positions/orientations and unseen objects differ from training; 
                        Conformal Prediction’s exchangeability assumption breaks, so results are diagnostic rather than guaranteed.</li>
                    <li><b>Label/regime mismatch.</b> The weak-training / strong-testing case can 
                        drop below F1=0.5 due to noisy supervision evaluated with strict labels under shift.</li>
                </ul>
                </div>
            </figure>

            <h4 class="title is-5">Does adding more data help?</h4>
            <p>
                Adding shifted data to increase training size did not consistently improve robustness. In fact, strongly supervised 
                transformers sometimes saw small drops in performance. Weakly supervised models benefited more when evaluated with 
                weak labels, but overall, label quality proved more important than sheer dataset size.
            </p>

            <h3 class="title is-4">Large In-Distribution Performance</h3>
            <figure>
                <img src="static/images/LID.svg" alt="Large-scale in-distribution results.">
                <div class="content" style="margin-top:10px">
                <ul class="is-size-6">
                    <li><b>Setting.</b> In-Distribution and Distribution Shift dataets are combined to increase training diversity while 
                        <i>maintaining exchangeability</i>, letting us assess whether scaling <i>within</i> 
                        distribution helps.</li>
                    <li><b>Takeaway.</b> Modeling the <i>temporal evolution</i> of token-level uncertainty with 
                        transformers is more predictive than static sequence-level scores, supporting sequential 
                        models for help detection.</li>
                </ul>
                </div>
            </figure>

            <h4 class="title is-5">How do models transfer across simulation OOD?</h4>
            <p>
                Despite training on real-world kitchen tuned &pi;<sub>0</sub>-FAST data, INSIGHT transferred surprisingly well to 
                simulation-based LIBERO tuned &pi;<sub>0</sub>-FAST data tests. Strongly supervised models, especially the jumbo variant, 
                reached accuracy and F1 approaching that of the sim-only benchmark. This shows that token-level uncertainty features are 
                robust across environments and policy checkpoints, suggesting that strong-label introspection modules can generalize without 
                retraining or re-annotation.
            </p>

            <h3 class="title is-4">Simulation OOD</h3>
            <figure>
                <img src="static/images/Sim_OOD.svg" alt="Simulation OOD results.">
                <div class="content" style="margin-top:10px">
                <ul class="is-size-6">
                    <li><b>Setting.</b> Train on real-world π<sub>0</sub>-FAST, test on a <i>different</i> π<sub>0</sub>-FAST 
                        fine-tuned on LIBERO → shift in tasks <i>and</i> policy behavior; a highly OOD stress test.</li>
                </ul>
                </div>
            </figure>

            <h4 class="title is-5">When and how often do models trigger help?</h4>
            <p>
                The strongly supervised model triggered help earliest and most frequently, maximizing failure coverage but risking 
                over-intervention. Weakly supervised models were conservative, rarely interrupting successes but sometimes missing 
                failures.
            </p>

            <h3 class="title is-4">Help Timing & Frequency</h3>
            <div class="table-card">
                <table class="table is-bordered is-striped is-hoverable is-fullwidth">
                    <thead>
                        <tr>
                            <th>Method</th>
                            <th>TTFH (fail) ↓</th>
                            <th>Triggers<sub>succ</sub> ↓</th>
                            <th>Triggers<sub>fail</sub> (≥ 1 ok)</th>
                            <th>Trigger Rate (success) ↓</th>
                            <th>Trigger Rate (fail) ↑</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>CP-W (Entropy)</td>
                            <td>6.891 ± 2.257</td>
                            <td>0.457 ± 0.302</td>
                            <td>1.721 ± 0.739</td>
                            <td>0.031 ± 0.020</td>
                            <td>0.118 ± 0.050</td>
                        </tr>
                        <tr>
                            <td><b>Strong Superv.</b></td>
                            <td><b>5.597 ± 0.809</b></td>
                            <td>0.710 ± 0.440</td>
                            <td><b>7.062 ± 1.225</b></td>
                            <td>0.047 ± 0.029</td>
                            <td><b>0.472 ± 0.081</b></td>
                        </tr>
                        <tr>
                            <td>Weak Superv.</td>
                            <td>7.929 ± 1.867</td>
                            <td><b>0.122 ± 0.172</b></td>
                            <td>1.566 ± 1.025</td>
                            <td><b>0.008 ± 0.011</b></td>
                            <td>0.105 ± 0.069</td>
                        </tr>
                    </tbody>
                </table>
                <p class="content has-text-justified" style="margin-top:10px">
                <b>What we measure.</b> Time-to-First-Help (TTFH), trigger counts on success vs. failure, 
                and per-step trigger rates characterize <i>when</i> and <i>how often</i> interventions occur.
                </p>
                <ul class="is-size-6">
                <li><b>Early vs. conservative.</b> Strong supervision fires <i>earlier</i> on failing episodes (lower TTFH) 
                    and more often during fails (higher fail-rate), at the cost of more triggers on successes; 
                    CP-W and Weak are more conservative (fewer success triggers) but react later/less on fails.</li>
                </ul>
            </div>
        </div>
    </section>

    <!-- Takeaways -->
    <!--section id="discussion" class="section">
    <div class="container is-max-desktop">
        <div class="eyebrow">Discussion</div>
        <h2 class="title is-3">Takeaways</h2>
        <p class="cap">
        What do these numbers mean for deployment? The Q&A below distills the main lessons from our experiments.
        </p>

        <div class="content">

        <h4 class="title is-5">How does aggregate vs. sequential use of metrics affect performance?</h4>
        <p>
            CP, which aggregates sequence-level scores, occasionally performed well under weak-label evaluation but generally 
            hovered near chance (accuracy/F1 close to 0.5). In contrast, <b>INSIGHT</b> leverages the sequential structure of token-level 
            signals via transformers, producing consistently higher accuracy and F1. This confirms that VLA uncertainty is inherently 
            temporal and requires sequence modeling.
        </p>

        <h4 class="title is-5">How do strong vs. weak labels compare?</h4>
        <p>
            <b>Strong labels</b> (step-level) provide fine-grained supervision and yield the most reliable models, but they are costly 
            and subjective to collect. <b>Weak labels</b> (episode-level success/failure) are scalable and objective, but noisier. 
            Strongly supervised transformers achieved the highest F1 in nearly all conditions, while weakly supervised models were 
            conservative—triggering less often, missing failures, but remaining competitive when evaluated under weak labels. 
            CP calibrated on weak labels outperformed in weak-label tests but lagged behind transformers overall.
        </p>
        </div>
    </div>
    </section-->

    <!--Conclusion-->
    <!--section id="conclusion" class="section">
        <div class="container is-max-desktop">
            <div class="eyebrow">Takeaways</div>
            <h2 class="title is-3">Conclusion & Future Directions</h2>

            <div class="content">
                <ul>
                    <li><strong>Token-level introspection works.:</strong> modeling temporal uncertainty with a compact
                        transformer yields earlier, more reliable help triggers than static scores.</li>
                    <li><strong>Supervision trade-off is important.:</strong> strong labels offer the best timing fidelity; weak
                        labels remain competitive when train/test granularity aligns—making large-scale deployment
                        practical.</li>
                    <li><strong>Selective intervention :</strong> INSIGHT reduces unnecessary interrupts on success
                        episodes while accelerating help on failures (lower TTFH, higher fail-trigger rate).</li>
                    <li><strong>Future directions:</strong> active data collection, adaptive thresholds, and
                        human-in-the-loop policies that leverage introspection for real-time recovery.</li>
                </ul>
            </div>
        </div>
    </section-->

    <!-- Takeaways -->
<section id="conclusion" class="section">
  <div class="container is-max-desktop">
    <div class="eyebrow">Takeaways</div>
    <h2 class="title is-3">What INSIGHT Shows</h2>

    <div class="content">
      <ul>
        <li><strong>Token-level introspection works.</strong> Modeling temporal uncertainty with a compact transformer yields earlier and more reliable help triggers than static, sequence-level scores.</li>

        <li><strong>Supervision quality matters more than size.</strong> Strong (step-level) labels deliver the highest F1 and most precise timing; adding more data helps less than improving label granularity.</li>

        <li><strong>Weak labels are viable when evaluation matches training.</strong> Episode-level (weak) supervision is scalable and remains competitive under weak-label tests, but underperforms with strict step-level evaluation.</li>

        <li><strong>Temporal modeling beats aggregation.</strong> Transformers over token sequences consistently outperform conformal prediction based on aggregated entropy/perplexity, especially outside perfectly aligned settings.</li>

        <li><strong>Choose your intervention style.</strong> Strongly-supervised models trigger earlier and more often on failures (lower TTFH, higher fail-rate) but may over-intervene; weak/CP variants are more conservative.</li>

        <li><strong>Generalization under shift is hard—but tractable.</strong> All methods degrade under distribution shift; strongly-supervised transformers degrade least and maintain better balance of precision/recall.</li>

        <li><strong>Transfer extends across domains and checkpoints.</strong> Models trained on real-world kitchen data transfer surprisingly well to LIBERO sim OOD; token-level uncertainty features remain stable across settings.</li>

        <li><strong>CP is a useful baseline, not a substitute.</strong> Conformal prediction can reach decent F1 when calibration and evaluation align, but lacks the temporal capacity needed for robust help triggering.</li>
      </ul>
    </div>

    <div class="content" style="margin-top:1rem">
      <h3 class="title is-5" style="margin-top:.6em">Where to go next</h3>
      <ul class="list-tight">
        <li><strong>Active supervision:</strong> use help-trigger uncertainty to prioritize annotation and data collection.</li>
        <li><strong>Adaptive thresholds:</strong> tune for safety vs. intrusiveness per task, user, or environment.</li>
        <li><strong>Human-in-the-loop recovery:</strong> integrate triggers with teleop/corrective feedback for real-time mitigation.</li>
      </ul>
    </div>
  </div>
</section>

    <!-- BibTex -->
    <section id="bibtex" class="section">
        <div class="container is-max-desktop">
            <h2 class="title is-3">BibTeX</h2>
            <p class="cap">
        If you found this useful, please cite INSIGHT using the entry below.
        </p>
            <pre style="white-space:pre-wrap;background:#f9fafb;border:1px solid #ddd;border-radius:8px;padding:14px;overflow:auto">
        @misc{karli2025insightinferencetimesequenceintrospection,
      title={INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models}, 
      author={Ulas Berk Karli and Ziyao Shangguan and Tesca FItzgerald},
      year={2025},
      eprint={2510.01389},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2510.01389}, 
}
            </pre>
            <p><a href="static/bibtex.bib">Download .bib</a></p>
        </div>
        </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>Website borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and
                            <a href="https://openvla.github.io/">OpenVLA</a> under <a
                                href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.</p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>